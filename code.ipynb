
# ============================================================================
# CELL 1: Install Dependencies
# ============================================================================
!pip install yfinance statsmodels scikit-learn matplotlib seaborn -q

print("âœ“ All dependencies installed successfully!")

# ============================================================================
# CELL 2: Import Libraries
# ============================================================================
import pandas as pd
import numpy as np
import yfinance as yf
from datetime import datetime, timedelta
import warnings
import json
warnings.filterwarnings('ignore')

# Modeling
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Colab specific
from google.colab import files
from IPython.display import display, HTML

plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

print("âœ“ All libraries imported successfully!")

# ============================================================================
# CELL 3: StockPredictionPipeline Class (FULLY CORRECTED)
# ============================================================================

class StockPredictionPipeline:
    """Complete pipeline for stock price prediction"""
    
    def __init__(self, tickers, start_date, end_date):
        self.tickers = tickers
        self.start_date = start_date
        self.end_date = end_date
        self.data = {}
        self.results = {}
        
    def fetch_data(self):
        """Task 1: Data Preparation"""
        print("=" * 80)
        print("TASK 1: DATA PREPARATION")
        print("=" * 80)
        
        for ticker in self.tickers:
            print(f"\nFetching data for {ticker}...")
            try:
                df = yf.download(ticker, start=self.start_date, end=self.end_date, progress=False)
                
                print(f"  Initial shape: {df.shape}")
                print(f"  Missing values: {df.isnull().sum().sum()}")
                
                if df.isnull().sum().sum() > 0:
                    print("  Handling missing values...")
                    df = df.fillna(method='ffill').fillna(method='bfill')
                
                df = df[~df.index.duplicated(keep='first')]
                
                if len(df) < 100:
                    print(f"  WARNING: Insufficient data for {ticker}")
                    continue
                    
                df = df[df['Volume'] > 0]
                
                for col in ['Open', 'High', 'Low', 'Close']:
                    mean = df[col].mean()
                    std = df[col].std()
                    df = df[np.abs(df[col] - mean) <= (5 * std)]
                
                print(f"  Final shape: {df.shape}")
                self.data[ticker] = df
                
            except Exception as e:
                print(f"  Error: {e}")
        
        print(f"\nâœ“ Successfully loaded {len(self.data)} stocks")
        return self
    
    def exploratory_data_analysis(self, ticker):
        """Task 2: EDA"""
        print("\n" + "=" * 80)
        print(f"TASK 2: EXPLORATORY DATA ANALYSIS - {ticker}")
        print("=" * 80)
        
        df = self.data[ticker].copy()
        
        print("\nDescriptive Statistics:")
        display(df.describe())
        
        df['Returns'] = df['Close'].pct_change()
        df['Log_Returns'] = np.log(df['Close'] / df['Close'].shift(1))
        
        price_change = float(((df['Close'].iloc[-1] / df['Close'].iloc[0]) - 1) * 100)
        volatility = float(df['Returns'].std())
        avg_return = float(df['Returns'].mean())
        
        print(f"\nOverall Price Change: {price_change:.2f}%")
        print(f"Volatility: {volatility:.4f}")
        print(f"Average Daily Return: {avg_return:.4f}")
        
        fig, axes = plt.subplots(3, 2, figsize=(15, 12))
        fig.suptitle(f'{ticker} - EDA', fontsize=16, fontweight='bold')
        
        axes[0, 0].plot(df.index, df['Close'], linewidth=2)
        axes[0, 0].set_title('Closing Price')
        axes[0, 0].set_ylabel('Price ($)')
        axes[0, 0].grid(True, alpha=0.3)
        
        axes[0, 1].plot(df.index, df['Volume'], alpha=0.7, color='steelblue')
        axes[0, 1].set_title('Volume')
        axes[0, 1].set_ylabel('Volume')
        axes[0, 1].grid(True, alpha=0.3)
        
        axes[1, 0].hist(df['Returns'].dropna(), bins=50, alpha=0.7, edgecolor='black')
        axes[1, 0].set_title('Returns Distribution')
        axes[1, 0].axvline(0, color='red', linestyle='--')
        axes[1, 0].grid(True, alpha=0.3)
        
        df['MA_20'] = df['Close'].rolling(window=20).mean()
        df['MA_50'] = df['Close'].rolling(window=50).mean()
        axes[1, 1].plot(df.index, df['Close'], label='Close', linewidth=2)
        axes[1, 1].plot(df.index, df['MA_20'], label='20-MA')
        axes[1, 1].plot(df.index, df['MA_50'], label='50-MA')
        axes[1, 1].set_title('Moving Averages')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        df['Volatility'] = df['Returns'].rolling(window=20).std()
        axes[2, 0].plot(df.index, df['Volatility'], color='orange', linewidth=2)
        axes[2, 0].set_title('20-day Volatility')
        axes[2, 0].grid(True, alpha=0.3)
        
        axes[2, 1].scatter(df['Volume'], df['Close'], alpha=0.5)
        axes[2, 1].set_title('Price vs Volume')
        axes[2, 1].set_xlabel('Volume')
        axes[2, 1].set_ylabel('Price ($)')
        axes[2, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'{ticker}_EDA.png', dpi=300, bbox_inches='tight')
        plt.show()
        print(f"âœ“ Saved {ticker}_EDA.png")
        
        return df
    
    def feature_engineering(self, df):
        """Task 3: Feature Engineering"""
        print("\n" + "=" * 80)
        print("TASK 3: FEATURE ENGINEERING")
        print("=" * 80)
        
        df = df.copy()
        
        # Lagged features
        for lag in [1, 2, 3, 5, 10]:
            df[f'Price_Lag_{lag}'] = df['Close'].shift(lag)
            df[f'Volume_Lag_{lag}'] = df['Volume'].shift(lag)
        
        # Rolling statistics
        for window in [5, 10, 20, 50]:
            df[f'MA_{window}'] = df['Close'].rolling(window=window).mean()
            df[f'STD_{window}'] = df['Close'].rolling(window=window).std()
            df[f'Volume_MA_{window}'] = df['Volume'].rolling(window=window).mean()
        
        # Price changes
        df['Price_Change'] = df['Close'].diff()
        df['Price_Change_Pct'] = df['Close'].pct_change()
        df['Volume_Change_Pct'] = df['Volume'].pct_change()
        
        # RSI
        delta = df['Close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df['RSI'] = 100 - (100 / (1 + rs))
        
        # MACD
        exp1 = df['Close'].ewm(span=12, adjust=False).mean()
        exp2 = df['Close'].ewm(span=26, adjust=False).mean()
        df['MACD'] = exp1 - exp2
        df['Signal_Line'] = df['MACD'].ewm(span=9, adjust=False).mean()
        
        # Bollinger Bands
        df['BB_Middle'] = df['Close'].rolling(window=20).mean()
        df['BB_Std'] = df['Close'].rolling(window=20).std()
        df['BB_Upper'] = df['BB_Middle'] + (2 * df['BB_Std'])
        df['BB_Lower'] = df['BB_Middle'] - (2 * df['BB_Std'])
        
        # High-Low range
        df['HL_Range'] = df['High'] - df['Low']
        df['HL_Range_Pct'] = (df['High'] - df['Low']) / df['Close']
        
        initial_len = len(df)
        df = df.dropna()
        dropped = initial_len - len(df)
        
        print(f"âœ“ Created {len(df.columns)} total columns")
        print(f"âœ“ Dropped {dropped} rows with NaN")
        print(f"âœ“ Final shape: {df.shape}")
        
        return df
    
    def train_arima_model(self, df, ticker):
        """Task 4a: ARIMA"""
        print("\n" + "=" * 80)
        print(f"TASK 4a: ARIMA MODEL - {ticker}")
        print("=" * 80)
        
        prices = df['Close'].values
        train_size = int(len(prices) * 0.8)
        train, test = prices[:train_size], prices[train_size:]
        
        print(f"Train: {len(train)}, Test: {len(test)}")
        
        # ACF/PACF plots
        fig, axes = plt.subplots(1, 2, figsize=(15, 5))
        plot_acf(train, lags=min(40, len(train)//2), ax=axes[0])
        plot_pacf(train, lags=min(40, len(train)//2), ax=axes[1])
        axes[0].set_title('ACF')
        axes[1].set_title('PACF')
        plt.tight_layout()
        plt.savefig(f'{ticker}_ACF_PACF.png', dpi=300)
        plt.show()
        
        # Grid search
        print("Finding optimal ARIMA parameters...")
        best_aic = np.inf
        best_order = None
        best_model = None
        
        for p in range(0, 3):
            for d in range(0, 2):
                for q in range(0, 3):
                    try:
                        model = ARIMA(train, order=(p, d, q))
                        fitted = model.fit()
                        if fitted.aic < best_aic:
                            best_aic = fitted.aic
                            best_order = (p, d, q)
                            best_model = fitted
                    except:
                        continue
        
        print(f"âœ“ Best order: {best_order}, AIC: {best_aic:.2f}")
        
        forecast = best_model.forecast(steps=len(test))
        
        rmse = np.sqrt(mean_squared_error(test, forecast))
        mae = mean_absolute_error(test, forecast)
        mape = np.mean(np.abs((test - forecast) / test)) * 100
        
        print(f"RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%")
        
        plt.figure(figsize=(15, 6))
        plt.plot(range(len(train)), train, label='Train', linewidth=2)
        plt.plot(range(len(train), len(train) + len(test)), test, 
                label='Actual', linewidth=2)
        plt.plot(range(len(train), len(train) + len(test)), forecast, 
                label='Forecast', linewidth=2, linestyle='--')
        plt.title(f'{ticker} - ARIMA Predictions')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(f'{ticker}_ARIMA.png', dpi=300)
        plt.show()
        
        return {
            'model': best_model,
            'order': best_order,
            'forecast': forecast,
            'actual': test,
            'metrics': {'RMSE': rmse, 'MAE': mae, 'MAPE': mape}
        }
    
    def train_gradient_boosting_model(self, df, ticker):
        """Task 4b: Gradient Boosting"""
        print("\n" + "=" * 80)
        print(f"TASK 4b: GRADIENT BOOSTING - {ticker}")
        print("=" * 80)
        
        feature_cols = [col for col in df.columns if col not in 
                       ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 
                        'Returns', 'Log_Returns', 'BB_Std']]
        
        X = df[feature_cols].values
        y = df['Close'].values
        
        train_size = int(len(X) * 0.8)
        X_train, X_test = X[:train_size], X[train_size:]
        y_train, y_test = y[:train_size], y[train_size:]
        
        print(f"Train: {len(X_train)}, Test: {len(X_test)}, Features: {X_train.shape[1]}")
        
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        print("Hyperparameter tuning...")
        param_grid = {
            'n_estimators': [100, 200],
            'max_depth': [3, 5],
            'learning_rate': [0.01, 0.1],
            'min_samples_split': [2, 5]
        }
        
        gb_model = GradientBoostingRegressor(random_state=42)
        grid_search = GridSearchCV(gb_model, param_grid, cv=3, 
                                   scoring='neg_mean_squared_error', 
                                   n_jobs=-1, verbose=0)
        grid_search.fit(X_train_scaled, y_train)
        
        print(f"âœ“ Best params: {grid_search.best_params_}")
        
        best_model = grid_search.best_estimator_
        y_pred = best_model.predict(X_test_scaled)
        
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        mae = mean_absolute_error(y_test, y_pred)
        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
        
        print(f"RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%")
        
        feature_importance = pd.DataFrame({
            'feature': feature_cols,
            'importance': best_model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print("\nTop 10 Features:")
        display(feature_importance.head(10))
        
        fig, axes = plt.subplots(2, 1, figsize=(15, 10))
        
        axes[0].plot(range(len(y_test)), y_test, label='Actual', linewidth=2)
        axes[0].plot(range(len(y_test)), y_pred, label='Predicted', 
                    linewidth=2, linestyle='--')
        axes[0].set_title(f'{ticker} - GB Predictions')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        top_features = feature_importance.head(15)
        axes[1].barh(range(len(top_features)), top_features['importance'])
        axes[1].set_yticks(range(len(top_features)))
        axes[1].set_yticklabels(top_features['feature'])
        axes[1].set_xlabel('Importance')
        axes[1].set_title('Feature Importance')
        axes[1].grid(True, alpha=0.3, axis='x')
        
        plt.tight_layout()
        plt.savefig(f'{ticker}_GradientBoosting.png', dpi=300)
        plt.show()
        
        return {
            'model': best_model,
            'scaler': scaler,
            'predictions': y_pred,
            'actual': y_test,
            'feature_importance': feature_importance,
            'metrics': {'RMSE': rmse, 'MAE': mae, 'MAPE': mape}
        }
    
    def compare_models(self, ticker, arima_results, gb_results):
        """Task 5: Model Evaluation"""
        print("\n" + "=" * 80)
        print(f"TASK 5: MODEL EVALUATION - {ticker}")
        print("=" * 80)
        
        comparison = pd.DataFrame({
            'Model': ['ARIMA', 'Gradient Boosting'],
            'RMSE': [arima_results['metrics']['RMSE'], gb_results['metrics']['RMSE']],
            'MAE': [arima_results['metrics']['MAE'], gb_results['metrics']['MAE']],
            'MAPE': [arima_results['metrics']['MAPE'], gb_results['metrics']['MAPE']]
        })
        
        print("\nModel Comparison:")
        display(comparison)
        
        rmse_imp = ((arima_results['metrics']['RMSE'] - gb_results['metrics']['RMSE']) / 
                    arima_results['metrics']['RMSE'] * 100)
        print(f"\nGB improvement: {rmse_imp:.2f}% better RMSE")
        
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        metrics = ['RMSE', 'MAE', 'MAPE']
        x = np.arange(len(metrics))
        width = 0.35
        
        axes[0].bar(x - width/2, comparison.iloc[0, 1:], width, label='ARIMA', alpha=0.8)
        axes[0].bar(x + width/2, comparison.iloc[1, 1:], width, label='GB', alpha=0.8)
        axes[0].set_xticks(x)
        axes[0].set_xticklabels(metrics)
        axes[0].set_title('Performance Comparison')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        min_len = min(len(arima_results['actual']), len(gb_results['actual']))
        axes[1].plot(range(min_len), arima_results['actual'][:min_len], 
                    label='Actual', linewidth=2)
        axes[1].plot(range(min_len), arima_results['forecast'][:min_len], 
                    label='ARIMA', linewidth=1.5, linestyle='--')
        axes[1].plot(range(min_len), gb_results['predictions'][:min_len], 
                    label='GB', linewidth=1.5, linestyle='--')
        axes[1].set_title('Predictions Overlay')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'{ticker}_Comparison.png', dpi=300)
        plt.show()
        
        return comparison
    
    def generate_report(self, ticker, arima_results, gb_results, comparison):
        """Task 6: Generate Report"""
        print("\n" + "=" * 80)
        print("TASK 6: GENERATING REPORT")
        print("=" * 80)
        
        report = f"""
{'='*80}
STOCK PRICE PREDICTION ANALYSIS
{'='*80}

Stock: {ticker}
Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

MODEL PERFORMANCE:
{comparison.to_string(index=False)}

IMPROVEMENT: GB is {((arima_results['metrics']['RMSE'] - gb_results['metrics']['RMSE']) / arima_results['metrics']['RMSE'] * 100):.1f}% better

TOP FEATURES:
{gb_results['feature_importance'].head(5).to_string(index=False)}

RECOMMENDATIONS:
1. Use Gradient Boosting for predictions
2. Set stop-loss at 2x RMSE: ${(gb_results['metrics']['RMSE'] * 2):.2f}
3. Retrain monthly with new data

{'='*80}
"""
        
        with open(f'{ticker}_Report.txt', 'w') as f:
            f.write(report)
        
        print(f"âœ“ Saved {ticker}_Report.txt")
        print(report)
        
        return report

print("âœ“ Pipeline class loaded!")

# ============================================================================
# CELL 4: Export Functions
# ============================================================================

def export_results_for_dashboard(pipeline, ticker):
    """Export for dashboard"""
    
    if ticker not in pipeline.results:
        return None
    
    print(f"\nExporting {ticker}...")
    
    results = pipeline.results[ticker]
    df = pipeline.data[ticker]
    
    historical_data = []
    hist_df = df.tail(60).copy()
    for idx, row in hist_df.iterrows():
        historical_data.append({
            'date': idx.strftime('%Y-%m-%d'),
            'actual': float(row['Close']),
            'volume': int(row['Volume'])
        })
    
    forecast_data = []
    actual_test = results['gradient_boosting']['actual']
    arima_pred = results['arima']['forecast']
    gb_pred = results['gradient_boosting']['predictions']
    
    min_len = min(len(actual_test), len(arima_pred), len(gb_pred))
    last_date = hist_df.index[-1]
    
    for i in range(min_len):
        forecast_date = last_date + timedelta(days=i+1)
        forecast_data.append({
            'date': forecast_date.strftime('%Y-%m-%d'),
            'actual': float(actual_test[i]),
            'arima': float(arima_pred[i]),
            'gradientBoosting': float(gb_pred[i])
        })
    
    feature_importance = []
    for idx, row in results['gradient_boosting']['feature_importance'].head(10).iterrows():
        feature_importance.append({
            'feature': row['feature'],
            'importance': float(row['importance'])
        })
    
    close_mean = float(df['Close'].mean())
    close_std = float(df['Close'].std())
    close_min = float(df['Close'].min())
    close_max = float(df['Close'].max())
    trend_pct = float(((df['Close'].iloc[-1] / df['Close'].iloc[0]) - 1) * 100)
    
    export_data = {
        'stock': ticker,
        'exportDate': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'historicalData': historical_data,
        'forecastData': forecast_data,
        'featureImportance': feature_importance,
        'metrics': {
            'arima': {
                'rmse': float(results['arima']['metrics']['RMSE']),
                'mae': float(results['arima']['metrics']['MAE']),
                'mape': float(results['arima']['metrics']['MAPE'])
            },
            'gradientBoosting': {
                'rmse': float(results['gradient_boosting']['metrics']['RMSE']),
                'mae': float(results['gradient_boosting']['metrics']['MAE']),
                'mape': float(results['gradient_boosting']['metrics']['MAPE'])
            }
        },
        'eda': {
            'statsData': [
                {'metric': 'Mean', 'value': f"{close_mean:.2f}"},
                {'metric': 'Std Dev', 'value': f"{close_std:.2f}"},
                {'metric': 'Min', 'value': f"{close_min:.2f}"},
                {'metric': 'Max', 'value': f"{close_max:.2f}"},
                {'metric': 'Trend', 'value': f"{trend_pct:+.1f}%"}
            ]
        },
        'modelInfo': {
            'arimaOrder': str(results['arima']['order']),
            'trainSize': len(historical_data),
            'testSize': len(forecast_data)
        }
    }
    
    filename = f'{ticker}_dashboard_data.json'
    with open(filename, 'w') as f:
        json.dump(export_data, f, indent=2)
    
    print(f"âœ“ Exported {filename}")
    
    return export_data


def export_all_stocks_for_dashboard(pipeline):
    """Export all stocks"""
    
    print("\n" + "="*80)
    print("EXPORTING ALL STOCKS FOR DASHBOARD")
    print("="*80)
    
    all_data = {}
    
    for ticker in pipeline.results.keys():
        stock_data = export_results_for_dashboard(pipeline, ticker)
        if stock_data:
            all_data[ticker] = stock_data
    
    filename = 'all_stocks_dashboard_data.json'
    with open(filename, 'w') as f:
        json.dump(all_data, f, indent=2)
    
    print(f"\nâœ“ Saved {filename}")
    print(f"âœ“ Total stocks: {len(all_data)}")
    
    print("\n" + "="*80)
    print("ðŸ“‹ COPY THIS JSON FOR DASHBOARD:")
    print("="*80)
    print(json.dumps(all_data, indent=2))
    print("="*80)
    
    return all_data

print("âœ“ Export functions loaded!")

# ============================================================================
# CELL 5: RUN THE COMPLETE ANALYSIS
# ============================================================================

# ============================================================================
# CELL 5: RUN THE COMPLETE ANALYSIS (CORRECTED)
# ============================================================================

# Configuration
tickers = ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'TSLA']
end_date = datetime.now()
start_date = end_date - timedelta(days=730)

print("="*80)
print("HEDGE FUND STOCK PRICE PREDICTION PIPELINE")
print("="*80)
print(f"\nStocks: {', '.join(tickers)}")
print(f"Period: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}")
print("\nEstimated time: 5-10 min per stock")
print("="*80 + "\n")

# Initialize
pipeline = StockPredictionPipeline(
    tickers=tickers,
    start_date=start_date.strftime('%Y-%m-%d'),
    end_date=end_date.strftime('%Y-%m-%d')
)

# Execute
try:
    pipeline.fetch_data()
    
    for ticker in pipeline.data.keys():
        print(f"\n{'#'*80}")
        print(f"# PROCESSING: {ticker}")
        print(f"{'#'*80}")
        
        # FIX: Use original data for each step, don't reuse EDA output
        df_eda = pipeline.data[ticker].copy()
        _ = pipeline.exploratory_data_analysis(ticker)  # Just for visualization
        
        # Use fresh copy for feature engineering
        df = pipeline.data[ticker].copy()
        df = pipeline.feature_engineering(df)
        
        # Check if we have enough data
        if len(df) < 100:
            print(f"âš ï¸ Insufficient data after feature engineering for {ticker}")
            continue
        
        arima_results = pipeline.train_arima_model(df, ticker)
        gb_results = pipeline.train_gradient_boosting_model(df, ticker)
        comparison = pipeline.compare_models(ticker, arima_results, gb_results)
        report = pipeline.generate_report(ticker, arima_results, gb_results, comparison)
        
        pipeline.results[ticker] = {
            'arima': arima_results,
            'gradient_boosting': gb_results,
            'comparison': comparison
        }
    
    print("\n" + "="*80)
    print("âœ… PIPELINE COMPLETED!")
    print("="*80)
    print(f"\nProcessed {len(pipeline.results)} stocks")
    
    # Export for dashboard
    all_dashboard_data = export_all_stocks_for_dashboard(pipeline)
    
    
except Exception as e:
    print(f"\nâŒ Error: {e}")
    import traceback
    traceback.print_exc()
